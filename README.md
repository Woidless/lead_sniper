**Обзор Проекта**

- **Назначение:**: `lead_sniper` — утилита для очистки, обогащения и загрузки данных компаний.
- **Язык:**: Python
- **Статус:**: Инструмент для локального использования / прототип

**Ключевые Функции**

- **Очистка данных:**: модуль `src.cleaner` содержит логику приведения входных CSV к единому формату.
- **Обогащение:**: модуль `src.enrich` выполняет дополнительные проверки и добавляет поля (например, по внешним источникам).
- **Загрузка:**: модуль `src.loader` загружает итоговые данные в целевой формат/хранилище (CSV/БД).
- **Интеграция с RusProfile:**: модуль `src.rusprofile` — парсер/интегратор для данных российских компаний.

**Требования**

- **Python:**: 3.8+ (проверьте совместимость с вашей средой)
- **Зависимости:**: указаны в файле `requirements.txt`

**Установка**

1. Клонируйте репозиторий или скачайте проект в рабочую директорию.
2. Создайте виртуальное окружение (рекомендуется):

```bash
python3 -m venv .venv
source .venv/bin/activate
```

3. Установите зависимости:

```bash
pip install -r requirements.txt
```

**Быстрый старт**

- **Запуск основного скрипта:**

```bash
python3 -m src.main
```


**Структура репозитория**

- **`src/`**: исходный код проекта
	- `__init__.py`
	- `cleaner.py` — очистка данных
	- `enrich.py` — логика обогащения
	- `loader.py` — сохранение/загрузка данных
	- `main.py` — точка входа
	- `rusprofile.py` — интеграция/парсер для RusProfile
- **`data/`**: примеры данных и сырые файлы
	- `companies.csv` — пример входного файла
	- `raw/` — необработанные/сырые дампы
- **`requirements.txt`** — зависимости Python
- **`lead_sniper.ipynb`** — ноутбук (опционально)

**Формат входных данных**

- **Ожидается CSV** с колонками, описывающими компанию (название, ИНН, ОГРН, адрес и т.д.).
- **Рекомендация:** сохранить исходный `data/companies.csv` и работать с копией.

**Как разрабатывать и отлаживать**

- **Стиль кода:** следуйте PEP8 и общим правилам проектирования Python-пакетов.
- **Запуск модуля вручную:** используйте `python -m src.main` для быстрой проверки.
- **Добавление логирования:** расширьте существующие вызовы логгера в `src/*` для детальной трассировки.

**Полезные команды**

```bash
# Установка зависимостей
pip install -r requirements.txt

# Запуск проекта (основной сценарий)
python3 -m src.main

# Запуск Python REPL с путём проекта (если нужно импортировать модули)
PYTHONPATH=./src python3
```

**Примеры использования**

### 1. Базовая обработка (очистка + обогащение)

```python
from src.cleaner import clean_company_data
from src.enrich import enrich_data

# Загрузите данные
input_file = "data/companies.csv"
df = pd.read_csv(input_file)

# Очистка
cleaned_df = clean_company_data(df)

# Обогащение
enriched_df = enrich_data(cleaned_df)

# Сохранение
enriched_df.to_csv("data/processed_companies.csv", index=False)
```

### 2. Интеграция с RusProfile

```python
from src.rusprofile import RusProfileParser

parser = RusProfileParser()
# Парсинг данных по ИНН/ОГРН
company_info = parser.fetch_company_info(inn="7701234567")
print(company_info)
```

### 3. Загрузка обработанных данных

```python
from src.loader import save_to_csv, save_to_database

# Сохранение в CSV
save_to_csv(processed_df, "data/output/companies_final.csv")

# Опционально — загрузка в БД
# save_to_database(processed_df, connection_string)
```

**Описание основных модулей**

#### `src/cleaner.py`
Выполняет первичную обработку данных:
- Удаление дубликатов
- Нормализация названий компаний
- Очистка и валидация ИНН/ОГРН
- Стандартизация адресов

#### `src/enrich.py`
Добавляет дополнительную информацию:
- Проверка статуса компании
- Получение дополнительных реквизитов из внешних источников
- Категоризация по отраслям
- Оценка качества данных

#### `src/loader.py`
Управляет сохранением результатов:
- Экспорт в различные форматы (CSV, JSON, SQL)
- Валидация перед сохранением
- Логирование ошибок при загрузке

#### `src/rusprofile.py`
Специализированный модуль для российских реквизитов:
- Парсинг данных с РусПрофиля
- Кеширование результатов
- Обработка ошибок сети

#### `src/main.py`
Главная точка входа проекта. Координирует работу всех модулей в единый pipeline.

**Советы по использованию**

- **Бекап данных:** перед массовой обработкой делайте бекап `data/raw/`.
- **Конфигурация:** если планируется множество параметров запуска, вынесите их в `config.yml` или окружение.
- **Логирование:** включите DEBUG-уровень логирования для отладки: `LOGLEVEL=DEBUG python3 -m src.main`
- **Тестирование на подмножестве:** сначала запустите на 10-100 строках для проверки логики.


**Требования к среде**

- **ОС:** Linux, macOS, Windows (рекомендуется WSL на Windows)
- **Python:** 3.8, 3.9, 3.10, 3.11+
- **Зависимости:** см. `requirements.txt`

Проверьте версию Python:
```bash
python3 --version
```


**ОТЧЕТ**
1.	Подход
	-	искал данных в разных сайтах с доступными данными, без подписок и избегая незаконных способ парсинг. сайты, которые проверил: https://www.tadviser.ru, https://focus.kontur.ru, https://www.rusprofile.ru, https://saby.ru, headhunter, https://www.list-org.com
	-	скачал данные о компаниях https://www.list-org.com в excel формате. Сайт имеет удобную поисковую систему с фильтрацией, а сами сведение имеют максимальную сводку о компании в сравнении с другими платформами. Сама фильтрация проходила по ОКВЭД IT кодам. Изучая что из себя представляет IT компания и какие компании могут называться IT, нашёл статью "В приложении 1 к постановлению № 1729 опубликован список из 31 кода. Для IT‑компаний чаще всего используются следующие из них: 26.20.4,46.51.2,58.2,62.01,62.02.1,62.02.4,62.02.9,62.03.11,62.03.13,62.09,63.11.1,63.12,74.90.9" и тем самым использовал как ориентир для фильтрации
	-	по фильтрации отметил от 100 до 500 сотрудников

2.	Что получилось / что не получилось
	-	получилсоь:
	1) Получил данные с https://www.list-org.com с нужнми данными, но без ОКВЭД. 
	2) очистил данные, дополнил данные, номрализовал
	3) стянул недостающие данные (ОКВЭД) парсигом
	4) очистил данные, дополнил, нормализовал
	5) выгрузил в файл data/comanies.csv
	-	не получилось:
	1) найти сайт с внятными описаниями каждой компании. в основном описании были характера "основан, кто гендиректор и тд.", но ничего в кратце не описывающего деятельность компании
	2) найти еще сайты, которые так же имели бы доступные данные как в "list-org.com". Основные проблемы с обходом анти-ботов и ограничений доступов без подписки
	3) не успел реализовать обход анти-ботов

3.	Роль LLM
	- Использовал ChatGHPT для:
	1) ускорение процессов нахождения нужных html разметок, чтобы получить данные (в данной работе это - определения ОКВЭД компании)
	2) объяснения ошибок. Зачастую я использу ChatGPT вместо поисковика, чтобы найти ошибку в коде/логике, но придерживаюсь того, чтобы писать код сам и избегать кодов, которые я сам не понимаю
	3) советоваться. К примеру, я спрашивал разьяснений что из себя представляют аббревиатуры ОКВЭД, ЕГРЮЛ и тд.
	4) для быстрой адаптации файла lead_sniper.ipynb к требованию оформления тестовой задачи. сам файл lead_sniper.ipynb содержит в себе изначальный код. там прописан весь процесс и видна ясно выстроенная цепочка действий (пожтому настоятельно рекомендую просмотретьь файл lead_sniper.ipynb) 
	- ИСпользовал GitHub Copilot, встроенный AI-помощник в VS Code для: 
	1) Для создания README.md использовал GitHub Copilot, встроенный AI-помощник в VS Code.
	